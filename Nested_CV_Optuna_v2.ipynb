{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import optuna\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score\nfrom optuna.samplers import TPESampler\nfrom catboost import CatBoostClassifier\nfrom optuna.pruners import BasePruner\nimport time\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n\nfrom optuna.integration import TFKerasPruningCallback\nimport keras\nfrom keras.layers import Dense\nfrom keras import Input\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nfrom optuna.trial import TrialState","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define custom transformer\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Select only specified columns.\"\"\"\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimoutPruner(BasePruner):\n    def __init__(self, max_sec_per_trial=2):\n        self._max_sec_per_trial = max_sec_per_trial\n\n    def prune(self, study, trial) -> bool:\n\n        step = trial.last_step\n        \n        if not step:\n            # initialize timestamp\n            self.start_time = time.time()\n\n        else:  # trial.last_step == None when no scores have been reported yet                \n            if time.time() - self.start_time > self._max_sec_per_trial:\n                print(f\"This trial takes more than {self._max_sec_per_trial} seconds.\")\n                return True\n\n        return False\n    \npruner_timeout = TimoutPruner(max_sec_per_trial=900)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(trial):\n    # We optimize the number of layers, hidden units and dropout in each layer and\n    # the learning rate of adam optimizer.\n\n    # We define our MLP.\n    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n    model = Sequential()\n    model.add(Input(shape=(X.shape[1],)))\n    for i in range(n_layers):\n        num_hidden = trial.suggest_int(\"n_units_l{}\".format(i), 4, 256, log=True)\n        model.add(Dense(num_hidden, activation=\"relu\"))\n        dropout = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n        model.add(Dropout(rate=dropout))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Scaler\ndef objective_optimize(trial, model_name= \"RandomForestClassifier\", scoring = \"f1_weighted\", timeout = False, cat_cols = [], num_cols =[],\n                      X=[], y=[], scaler_m = RobustScaler()):\n    \n    categorical = cat_cols\n    numerical = num_cols\n    \n    # Define categorical pipeline\n    cat_pipe = Pipeline([\n        ('selector', ColumnSelector(categorical)),\n    ])\n    # Define numerical pipeline\n    num_pipe_rb = Pipeline([\n        ('selector', ColumnSelector(numerical)),\n        ('scaler', scaler_m)\n    ])\n\n    # Fit feature union to training data\n    preprocessor = FeatureUnion([\n        ('cat', cat_pipe),\n        ('num', num_pipe_rb)\n    ])\n\n    # create pipeline\n    estimators = []\n    estimators.append(('feature_union', preprocessor))\n        \n    cv_outer=StratifiedKFold(n_splits=5, random_state=1,shuffle=True)\n    \n    if model_name == \"RandomForestClassifier\":\n        param = {        \n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 10000, step=100),\n            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 1, 24),\n            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 1, 24),\n            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 12),\n            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 12),\n            \"n_jobs\": -1,\n        }\n        \n        estimators.append(('RF', RandomForestClassifier(**param)))\n        pipe = Pipeline(estimators)\n        \n    if model_name == \"LGBMClassifier\":\n        param = {\n        \"objective\": \"binary\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"n_jobs\": -1,\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 10.0, step=0.01),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 10.0, step=0.01),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 150),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 48),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1, step=0.01),    \n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5000, step=200),    \n        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.001, 1, step=0.1),   \n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.01, 0.5, log=True),\n        }\n            \n        estimators.append(('LGBM', LGBMClassifier(**param)))\n        pipe = Pipeline(estimators)\n        \n    if model_name == \"CatBoostClassifier\":\n        param = {\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.5, log=True),\n        \"depth\": trial.suggest_int(\"depth\", 1, 34),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 10.0, step=0.01),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 32),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 5000, step=400), \n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 1, step=0.01),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 55),  \n        \"eval_metric\": \"AUC\",\n        \"scale_pos_weight\": ratio,\n        \"early_stopping_rounds\": 100\n        }\n            \n        estimators.append(('CB', CatBoostClassifier(**param)))\n        pipe = Pipeline(estimators)\n        \n    if model_name== \"Keras\":\n        rc = scaler_m\n        X[numerical] = rc.fit_transform(X[numerical])\n        X[categorical] = rc.fit_transform(X[categorical])\n    \n        n_splits = 5\n        cv_outer=StratifiedKFold(n_splits=n_splits, random_state=1,shuffle=True)\n        x_train, x_valid, y_train, y_valid = train_test_split(X,y,test_size=0.20,random_state=101)\n    \n        METRIC_NAME = \"AUC_Watcher\"\n        metrics=[tf.keras.metrics.AUC(name=METRIC_NAME), \"accuracy\"]\n    \n        # We compile our model with a sampled learning rate.\n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n        model.compile(\n            loss=\"binary_crossentropy\",\n            optimizer=Adam(learning_rate=learning_rate),\n            metrics= metrics,\n        )\n    \n        checkpointer = ModelCheckpoint(filepath = '', verbose=1, save_best_only=True, save_weights_only = False)\n    \n        callbacks = [\n            TFKerasPruningCallback(trial, \"val_AUC_Watcher\"),\n            checkpointer\n        ]\n\n        # Fit the model on the training data.\n        # The KerasPruningCallback checks for pruning condition every epoch.\n        val_AUC = []\n        for train_idx, val_idx in cv_outer.split(X, y):\n            X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n            X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n            \n            # Generate our trial model.\n            model = create_model(trial)\n            \n            history = model.fit(\n                X_train_fold,\n                y_train_fold,\n                batch_size=64,\n                callbacks=callbacks,\n                epochs=50,\n                validation_data=(X_val_fold, y_val_fold),\n                verbose=1,\n            )\n\n            val_AUC.append(np.mean(history.history[\"val_AUC_Watcher\"]))\n\n        return sum(val_AUC) / n_splits\n    \n    if timeout:\n        for step in range(5):    \n            time.sleep(trial.number)\n            cvs_best = cross_val_score(pipe, X, y,cv=cv_outer,scoring= scoring).mean()\n            trial.report(cvs_best, step)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    else:\n        cvs_best = cross_val_score(pipe, X, y,cv=cv_outer,scoring= scoring).mean()\n    \n    return cvs_best","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels=[]\nscores=[]\nhyperparams={}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_outer_split = StratifiedKFold(n_splits=5, random_state=1,shuffle=True)\nnested_score = []\n\nfor train_index, test_index in cv_outer_split.split(X, y):\n    # split data into train and test sets\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    # define study object for hyperparameter optimization\n    func = lambda trial: objective_optimize(trial, model_name= \"LGBMClassifier\",  timeout = True, cat_cols = cat_cols, num_cols = num_cols, X=X, y=y)\n    study = optuna.create_study(direction='maximize',\n                                pruner=pruner_timeout,\n                                sampler=TPESampler())\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    study.optimize(func, n_trials=1, timeout = 20000, show_progress_bar = True)\n    \n    # initialize SVR model with optimized hyperparameters\n    LGBM_model = LGBMClassifier(**study.best_params)\n\n    # fit SVR model on inner cross-validation data\n    LGBM_model.fit(X_train, y_train)\n\n    # calculate score on outer cross-validation data\n    nested_score.append(LGBM_model.score(X_test, y_test))\n    \n# print mean score and standard deviation\nprint(\"Nested CV score: %.4f +/- %.4f\" % (np.mean(nested_score), np.std(nested_score)))\n\nmodel='LGBM'\nscore=study.best_trial.value\nmodels.append(model)\nscores.append(score)\nhyperparams[model] = study.best_trial.params","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}